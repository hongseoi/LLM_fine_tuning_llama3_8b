{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8434268,"sourceType":"datasetVersion","datasetId":5023400},{"sourceId":8436265,"sourceType":"datasetVersion","datasetId":5024842}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qqq -U transformers datasets accelerate peft trl bitsandbytes deepspeed --progress-bar off","metadata":{"execution":{"iopub.status.busy":"2024-05-17T20:04:21.683385Z","iopub.execute_input":"2024-05-17T20:04:21.683780Z","iopub.status.idle":"2024-05-17T20:05:13.856297Z","shell.execute_reply.started":"2024-05-17T20:04:21.683750Z","shell.execute_reply":"2024-05-17T20:05:13.855270Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import gc\nimport os\nimport json\nfrom kaggle_secrets import UserSecretsClient","metadata":{"execution":{"iopub.status.busy":"2024-05-17T20:05:43.637025Z","iopub.execute_input":"2024-05-17T20:05:43.637398Z","iopub.status.idle":"2024-05-17T20:05:43.656631Z","shell.execute_reply.started":"2024-05-17T20:05:43.637364Z","shell.execute_reply":"2024-05-17T20:05:43.655642Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Get keys from Secrets\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-05-17T20:05:46.416251Z","iopub.execute_input":"2024-05-17T20:05:46.416620Z","iopub.status.idle":"2024-05-17T20:05:46.643635Z","shell.execute_reply.started":"2024-05-17T20:05:46.416592Z","shell.execute_reply":"2024-05-17T20:05:46.642891Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/ds-config/ds_config_zero3.json') as f:\n    ds_config = json.load(f)\nds_config_dict=dict(zero3=ds_config)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T20:06:05.404496Z","iopub.execute_input":"2024-05-17T20:06:05.404918Z","iopub.status.idle":"2024-05-17T20:06:05.419201Z","shell.execute_reply.started":"2024-05-17T20:06:05.404885Z","shell.execute_reply":"2024-05-17T20:06:05.418438Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"ds_config_dict[\"zero3\"]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T20:06:07.724477Z","iopub.execute_input":"2024-05-17T20:06:07.725095Z","iopub.status.idle":"2024-05-17T20:06:07.734912Z","shell.execute_reply.started":"2024-05-17T20:06:07.725062Z","shell.execute_reply":"2024-05-17T20:06:07.733849Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'fp16': {'enabled': 'auto',\n  'loss_scale': 0,\n  'loss_scale_window': 1000,\n  'initial_scale_power': 16,\n  'hysteresis': 2,\n  'min_loss_scale': 1},\n 'bf16': {'enabled': 'auto'},\n 'optimizer': {'type': 'AdamW',\n  'params': {'lr': 'auto',\n   'weight_decay': 'auto',\n   'torch_adam': True,\n   'adam_w_mode': True}},\n 'scheduler': {'type': 'WarmupLR',\n  'params': {'warmup_min_lr': 'auto',\n   'warmup_max_lr': 'auto',\n   'warmup_num_steps': 'auto'}},\n 'zero_optimization': {'stage': 3,\n  'offload_optimizer': {'device': 'cpu', 'pin_memory': True},\n  'offload_param': {'device': 'cpu', 'pin_memory': True},\n  'overlap_comm': True,\n  'contiguous_gradients': True,\n  'sub_group_size': 1000000000.0,\n  'reduce_bucket_size': 'auto',\n  'stage3_prefetch_bucket_size': 'auto',\n  'stage3_param_persistence_threshold': 'auto',\n  'stage3_max_live_parameters': 1000000000.0,\n  'stage3_max_reuse_distance': 1000000000.0,\n  'stage3_gather_16bit_weights_on_model_save': 'auto'},\n 'gradient_accumulation_steps': 1,\n 'gradient_clipping': 'auto',\n 'steps_per_print': 2000,\n 'train_batch_size': 'auto',\n 'train_micro_batch_size_per_gpu': 'auto',\n 'wall_clock_breakdown': False}"},"metadata":{}}]},{"cell_type":"code","source":"import os\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()  # Write a config file\nos._exit(00)  # Restart the notebook","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"CUDA Version: {torch.version.cuda}\")\nprint(f\"Pytorch {torch.__version__}\")\n\n# Check the type and quantity of GPUs\nif torch.cuda.is_available():\n    print('Num CPUs:', os.cpu_count())\n    print('Num GPUs:', torch.cuda.device_count())\n    print('GPU Type:', torch.cuda.get_device_name(0))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:47:01.182335Z","iopub.execute_input":"2024-05-17T22:47:01.183079Z","iopub.status.idle":"2024-05-17T22:47:01.189879Z","shell.execute_reply.started":"2024-05-17T22:47:01.183043Z","shell.execute_reply":"2024-05-17T22:47:01.188848Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Device: cuda\nCUDA Version: 12.1\nPytorch 2.1.2\nNum CPUs: 4\nNum GPUs: 2\nGPU Type: Tesla T4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Llama_3_8b","metadata":{}},{"cell_type":"code","source":"# Model\nbase_model = \"meta-llama/Meta-Llama-3-8B\"","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:41:03.666730Z","iopub.execute_input":"2024-05-17T22:41:03.667433Z","iopub.status.idle":"2024-05-17T22:41:03.671598Z","shell.execute_reply.started":"2024-05-17T22:41:03.667401Z","shell.execute_reply":"2024-05-17T22:41:03.670684Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=42).select(range(100)) # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc= os.cpu_count(),\n)\ndataset = dataset.train_test_split(test_size=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from accelerate import notebook_launcher\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T20:06:16.288211Z","iopub.execute_input":"2024-05-17T20:06:16.289034Z","iopub.status.idle":"2024-05-17T20:06:20.311359Z","shell.execute_reply.started":"2024-05-17T20:06:16.289005Z","shell.execute_reply":"2024-05-17T20:06:20.310443Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\ndef main():\n    \n    from transformers import BitsAndBytesConfig\n    from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n    from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n    from accelerate import Accelerator\n\n#     from accelerate import FullyShardedDataParallelPlugin\n#     from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n\n#     fsdp_plugin = FullyShardedDataParallelPlugin(\n#         state_dict_config=FullStateDictConfig(offload_to_cpu=False, rank0_only=False),\n#         optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=False, rank0_only=False),\n#     )\n\n#     accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n\n    from accelerate import Accelerator, DeepSpeedPlugin\n    deepspeed_plugin = DeepSpeedPlugin(\n            hf_ds_config=ds_config_dict[\"zero3\"],\n            gradient_accumulation_steps=4,\n            gradient_clipping=1.0,\n            zero_stage=3,\n            offload_optimizer_device=None,\n            offload_param_device=None,\n            zero3_save_16bit_model=True,\n            zero3_init_flag=True,\n        )\n    \n    kwargs = {\n        \"fp16.enabled\": True, \n        \"fp16.auto_cast\": False,\n        \"bf16.enabled\": False,\n        \"optimizer.params.lr\": 8e-6,\n        \"optimizer.params.weight_decay\": 0.0,\n        \"scheduler.params.warmup_min_lr\": 0.0,\n        \"scheduler.params.warmup_max_lr\": 5e-5,\n        \"scheduler.params.warmup_num_steps\": 0,\n        \"train_micro_batch_size_per_gpu\": 1,\n        \"gradient_clipping\": 1.0,\n        \"train_batch_size\": 1,\n        \"zero_optimization.reduce_bucket_size\": 5e5,\n        \"zero_optimization.stage3_prefetch_bucket_size\": 5e5,\n        \"zero_optimization.stage3_param_persistence_threshold\": 5e5,\n#         \"zero_optimization.stage3_gather_16bit_weights_on_model_save\": False,\n    }\n    deepspeed_plugin.deepspeed_config_process(**kwargs)\n    \n    accelerator = Accelerator(deepspeed_plugin=deepspeed_plugin, mixed_precision=\"fp16\")\n#     accelerator = Accelerator(mixed_precision='fp16')\n#     accelerator = Accelerator()\n    \n    device_map = {\"\": accelerator.process_index}\n#     device_map = {\"\": \"cuda:\" + str(int(os.environ.get(\"LOCAL_RANK\") or 0))}\n#     device_map={'':torch.cuda.current_device()}\n\n    \n    # QLoRA config\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_storage=torch.float16,\n    )\n\n    # LoRA config\n    peft_config = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        lora_dropout=0.1,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n#         target_modules=[\"all_linear\"],\n        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n    )\n    \n    base_model = \"meta-llama/Meta-Llama-3-8B\"\n    new_model = \"Llama-3-8B_FT_ORPO_9k\"\n    \n    tokenizer = AutoTokenizer.from_pretrained(base_model, token=HF_TOKEN)\n    \n#     tokenizer.pad_token = tokenizer.eos_token\n\n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        quantization_config=bnb_config,\n#         device_map=\"auto\",\n#         device_map=device_map,\n        token=HF_TOKEN,\n        attn_implementation=\"eager\",\n        torch_dtype=torch.float16,\n    )\n    \n    model, tokenizer = setup_chat_format(model, tokenizer)\n    model = prepare_model_for_kbit_training(model)\n    \n    dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n    dataset = load_dataset(dataset_name, split=\"all\")\n    dataset = dataset.shuffle(seed=42).select(range(900)) # Only use 30 samples for test\n\n    def format_chat_template(row):\n        row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n        row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n        return row\n\n    dataset = dataset.map(\n        format_chat_template,\n        num_proc= os.cpu_count(),\n    )\n    dataset = dataset.train_test_split(test_size=0.01)\n    \n#     torch.cuda.empty_cache()\n    \n    orpo_args = ORPOConfig(\n        learning_rate=8e-6,\n        lr_scheduler_type=\"linear\",\n        max_length=1024,\n        max_prompt_length=512,\n        beta=0.1,\n        per_device_train_batch_size=1,\n        per_device_eval_batch_size=1,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_8bit\",\n        num_train_epochs=1,\n        evaluation_strategy=\"steps\",\n#         eval_strategy=\"steps\",\n        eval_steps=0.2,\n        logging_steps=1,\n        warmup_steps=10,\n        report_to=\"none\",\n        output_dir=\"./results/\",\n        remove_unused_columns=False,\n#         fp16=True,\n#         bf16=False,\n#         fsdp=\"full_shard\",\n#         fsdp_config={'min_num_params': 2000, 'offload_params': False, 'sharding_strategy': 1},\n        ddp_find_unused_parameters=False,\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs = {\"use_reentrant\": True}, #must be false for DDP\n    )\n\n    trainer = ORPOTrainer(\n        model=model,\n        args=orpo_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        peft_config=peft_config,\n        tokenizer=tokenizer,\n    )\n\n    print(device_map)\n    print(f'n_gpu: {orpo_args.n_gpu}; Mode: {orpo_args.parallel_mode}')\n    print(f'Num Processes: {accelerator.num_processes}; Device: {accelerator.device}; Process Index: {accelerator.process_index}')\n    print(f'Accel Type: {accelerator.distributed_type}')\n\n    \n    trainer.train()\n    trainer.save_model(new_model)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-17T20:07:02.953707Z","iopub.execute_input":"2024-05-17T20:07:02.954260Z","iopub.status.idle":"2024-05-17T20:07:02.974904Z","shell.execute_reply.started":"2024-05-17T20:07:02.954233Z","shell.execute_reply":"2024-05-17T20:07:02.973880Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%%time\n\nnotebook_launcher(main, num_processes=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T20:07:08.212401Z","iopub.execute_input":"2024-05-17T20:07:08.213266Z","iopub.status.idle":"2024-05-17T22:34:41.455490Z","shell.execute_reply.started":"2024-05-17T20:07:08.213233Z","shell.execute_reply":"2024-05-17T22:34:41.453492Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Launching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"2024-05-17 20:07:11.165676: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-17 20:07:11.165676: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-17 20:07:11.165739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-17 20:07:11.165794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-17 20:07:11.322840: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-17 20:07:11.322816: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[2024-05-17 20:07:22,860] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2024-05-17 20:07:22,860] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n\n\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\n\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\n\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/compiler_compat/ld: cannot find -laio: /No optsuch/ condafile/ compiler_compator/ lddirectory:\n cannot find -laio: No such file or directory\ncollect2: error: ld returned 1 exit status\ncollect2: error: ld returned 1 exit status\n","output_type":"stream"},{"name":"stdout","text":"[2024-05-17 20:07:23,445] [INFO] [comm.py:637:init_distributed] cdb=None\n[2024-05-17 20:07:23,446] [INFO] [comm.py:637:init_distributed] cdb=None\n[2024-05-17 20:07:23,447] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09c406cdf36d462398a99879d93d00cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c95f9069eb7452fbb61663c1b0396d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f8dd5ee462431bb161eee6066a95d8"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"541935240d964571aedabe1d9aebd90a"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d1d26659ba442abdc8c8a9d48f73c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e198534a034c259da6af73b283e8da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c4b60f6daee47519b51dae69106018e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb46bf2bf87b47c89385be09f9f7da74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1c61249545342918decc28556e55a24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ed4b00283e456caa06e116623987ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee1d71c442704867b65269d5b355f21d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eaca21682d0481cb0ee0c5c297e2c7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4407cbf5008f438d8a436e1f32c40b1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd91dc9fe83c42708587006ce1be9b79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/2.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59263b907ec74bd2b28fea42acbb7ed1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/115M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cbc57a8eab64a9f9e48b8a7aa02ba9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/44245 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8787992d096c44569dcfa2d8aa5ebefc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d508c74173a64d69935d80223f9c519e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af859a31b1f4bd6b0a534b86ead459a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/891 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8d4bdce0eab4da095efd2d73594f0ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa74a9b1983149aa95c4591079bed29b"}},"metadata":{}},{"name":"stdout","text":"{'': 0}\nn_gpu: 1; Mode: ParallelMode.DISTRIBUTED\nNum Processes: 2; Device: cuda:0; Process Index: 0\nAccel Type: DEEPSPEED\n{'': 1}\nn_gpu: 1; Mode: ParallelMode.DISTRIBUTED\nNum Processes: 2; Device: cuda:1; Process Index: 1\nAccel Type: DEEPSPEED\n[2024-05-17 20:09:59,655] [WARNING] [engine.py:1188:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n","output_type":"stream"},{"name":"stderr","text":"Could not estimate the number of tokens of the input, floating-point operations will not be computed\nCould not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [111/111 2:23:31, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n      <th>Steps Per Second</th>\n      <th>Rewards/chosen</th>\n      <th>Rewards/rejected</th>\n      <th>Rewards/accuracies</th>\n      <th>Rewards/margins</th>\n      <th>Logps/rejected</th>\n      <th>Logps/chosen</th>\n      <th>Logits/rejected</th>\n      <th>Logits/chosen</th>\n      <th>Nll Loss</th>\n      <th>Log Odds Ratio</th>\n      <th>Log Odds Chosen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>23</td>\n      <td>3.521300</td>\n      <td>4.612779</td>\n      <td>32.903700</td>\n      <td>0.274000</td>\n      <td>0.152000</td>\n      <td>-0.365334</td>\n      <td>-0.322751</td>\n      <td>0.400000</td>\n      <td>-0.042582</td>\n      <td>-3.227514</td>\n      <td>-3.653337</td>\n      <td>-1.626554</td>\n      <td>-1.480288</td>\n      <td>4.911569</td>\n      <td>-0.983836</td>\n      <td>-0.455553</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>3.035500</td>\n      <td>3.803152</td>\n      <td>32.819400</td>\n      <td>0.274000</td>\n      <td>0.152000</td>\n      <td>-0.309846</td>\n      <td>-0.270180</td>\n      <td>0.400000</td>\n      <td>-0.039667</td>\n      <td>-2.701799</td>\n      <td>-3.098464</td>\n      <td>-1.863598</td>\n      <td>-1.721089</td>\n      <td>3.758012</td>\n      <td>-0.981110</td>\n      <td>-0.448183</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>2.750900</td>\n      <td>3.280714</td>\n      <td>33.013700</td>\n      <td>0.273000</td>\n      <td>0.151000</td>\n      <td>-0.275840</td>\n      <td>-0.240344</td>\n      <td>0.200000</td>\n      <td>-0.035496</td>\n      <td>-2.403441</td>\n      <td>-2.758399</td>\n      <td>-2.068769</td>\n      <td>-1.894428</td>\n      <td>2.879775</td>\n      <td>-0.976350</td>\n      <td>-0.434299</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>2.965100</td>\n      <td>2.717235</td>\n      <td>33.026000</td>\n      <td>0.273000</td>\n      <td>0.151000</td>\n      <td>-0.233753</td>\n      <td>-0.208966</td>\n      <td>0.600000</td>\n      <td>-0.024787</td>\n      <td>-2.089662</td>\n      <td>-2.337535</td>\n      <td>-1.889880</td>\n      <td>-1.685683</td>\n      <td>2.470328</td>\n      <td>-0.900982</td>\n      <td>-0.305739</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [111/111 2:23:31, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n      <th>Steps Per Second</th>\n      <th>Rewards/chosen</th>\n      <th>Rewards/rejected</th>\n      <th>Rewards/accuracies</th>\n      <th>Rewards/margins</th>\n      <th>Logps/rejected</th>\n      <th>Logps/chosen</th>\n      <th>Logits/rejected</th>\n      <th>Logits/chosen</th>\n      <th>Nll Loss</th>\n      <th>Log Odds Ratio</th>\n      <th>Log Odds Chosen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>23</td>\n      <td>3.521300</td>\n      <td>4.612779</td>\n      <td>32.907400</td>\n      <td>0.273000</td>\n      <td>0.152000</td>\n      <td>-0.433638</td>\n      <td>-0.425617</td>\n      <td>0.600000</td>\n      <td>-0.008021</td>\n      <td>-4.256167</td>\n      <td>-4.336378</td>\n      <td>-1.928928</td>\n      <td>-1.985342</td>\n      <td>4.818929</td>\n      <td>-0.762590</td>\n      <td>-0.077687</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>3.035500</td>\n      <td>3.803152</td>\n      <td>32.822900</td>\n      <td>0.274000</td>\n      <td>0.152000</td>\n      <td>-0.376735</td>\n      <td>-0.366885</td>\n      <td>0.600000</td>\n      <td>-0.009850</td>\n      <td>-3.668853</td>\n      <td>-3.767350</td>\n      <td>-2.447012</td>\n      <td>-2.318133</td>\n      <td>4.089953</td>\n      <td>-0.776574</td>\n      <td>-0.095637</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>2.750900</td>\n      <td>3.280714</td>\n      <td>33.015500</td>\n      <td>0.273000</td>\n      <td>0.151000</td>\n      <td>-0.369881</td>\n      <td>-0.349238</td>\n      <td>0.400000</td>\n      <td>-0.020643</td>\n      <td>-3.492380</td>\n      <td>-3.698811</td>\n      <td>-2.561463</td>\n      <td>-2.327574</td>\n      <td>3.819503</td>\n      <td>-0.857361</td>\n      <td>-0.199253</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>2.965100</td>\n      <td>2.717235</td>\n      <td>33.029200</td>\n      <td>0.272000</td>\n      <td>0.151000</td>\n      <td>-0.299139</td>\n      <td>-0.290305</td>\n      <td>0.600000</td>\n      <td>-0.008834</td>\n      <td>-2.903047</td>\n      <td>-2.991386</td>\n      <td>-2.227511</td>\n      <td>-1.786906</td>\n      <td>3.145942</td>\n      <td>-0.773504</td>\n      <td>-0.077684</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6647db80-65754dc07b5e6c281a1dddce;b38fe6f5-e03f-4164-8db1-0dd5e5d22e60)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 1.92 s, sys: 822 ms, total: 2.75 s\nWall time: 2h 27min 33s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Merge Adapter with Base model","metadata":{}},{"cell_type":"code","source":"# Flush memory\ndel trainer, model\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reload tokenizer and model\n\nfrom trl import setup_chat_format\n\ntokenizer = AutoTokenizer.from_pretrained(base_model, token=HF_TOKEN)\nfp16_model = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    token=HF_TOKEN,\n)\nfp16_model, tokenizer = setup_chat_format(fp16_model, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:41:15.997547Z","iopub.execute_input":"2024-05-17T22:41:15.997920Z","iopub.status.idle":"2024-05-17T22:41:58.117008Z","shell.execute_reply.started":"2024-05-17T22:41:15.997893Z","shell.execute_reply":"2024-05-17T22:41:58.116014Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2946230fdca841a19596004d40d1112a"}},"metadata":{}}]},{"cell_type":"code","source":"# merge fine tuned adapter\nfrom peft import PeftModel\n\nnew_model = '/kaggle/working/Llama-3-8B_FT_ORPO_9k'\n\n# Merge adapter with base model\nmodel = PeftModel.from_pretrained(fp16_model, new_model)\nmodel = model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:42:13.298518Z","iopub.execute_input":"2024-05-17T22:42:13.299384Z","iopub.status.idle":"2024-05-17T22:42:14.306056Z","shell.execute_reply.started":"2024-05-17T22:42:13.299352Z","shell.execute_reply":"2024-05-17T22:42:14.305201Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:42:17.317245Z","iopub.execute_input":"2024-05-17T22:42:17.318163Z","iopub.status.idle":"2024-05-17T22:42:17.327135Z","shell.execute_reply.started":"2024-05-17T22:42:17.318130Z","shell.execute_reply":"2024-05-17T22:42:17.326138Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128258, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128258, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Inference with Fine-tuned model","metadata":{}},{"cell_type":"code","source":"%%time\nquestion = 'What is the basic structure of a SQL query to join to tables on a field like ID'\n# question = 'When is labor day celebrated in USA'\n\n# Tokenize the prompt\ninputs = tokenizer(question, return_tensors=\"pt\").to(DEVICE)\n# Generate the outputs from prompt\ngenerate_ids = model.generate(**inputs, max_new_tokens=256)\n# Decode the generated output\ngenerated_text = tokenizer.batch_decode(generate_ids,\n                                    skip_special_tokens=True,\n                                    clean_up_tokenization_spaces=False)[0]\n\nprint('generated_text: ', generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:48:10.010220Z","iopub.execute_input":"2024-05-17T22:48:10.011100Z","iopub.status.idle":"2024-05-17T22:48:27.636737Z","shell.execute_reply.started":"2024-05-17T22:48:10.011069Z","shell.execute_reply":"2024-05-17T22:48:27.635802Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"generated_text:  What is the basic structure of a SQL query to join to tables on a field like ID?\nI have two tables, let's call them A and B.\nA has a column called ID.\nB has a column called ID.\nI want to join these two tables on the ID field. How do I do this?\nThe basic syntax is:\nSELECT  * FROM  A\nINNER JOIN  B  ON  A.ID = B.ID\nThis will return all rows from A and B where the ID field is the same in both tables.\nIf you want to return only rows where the ID field is the same in both tables, you can use INNER JOIN instead of JOIN .\nTo return only rows where the ID field is different, you can use LEFT JOIN instead of JOIN .\nIf you want to return all rows from A and only rows from B where the ID field is the same in both tables, you can use RIGHT JOIN instead of JOIN .\nTo return only rows where the ID field is different, you can use LEFT JOIN instead of JOIN .\nTo return only rows where the ID field is different, you can use RIGHT JOIN instead of JOIN .\nTo return only rows where the ID field is different, you can use LEFT JOIN instead of JOIN .\nTo return only rows where the ID field is different, you can use RIGHT JOIN instead of JOIN .\nTo return only rows\nCPU times: user 17.6 s, sys: 0 ns, total: 17.6 s\nWall time: 17.6 s\n","output_type":"stream"}]}]}